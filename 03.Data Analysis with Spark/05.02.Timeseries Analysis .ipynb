{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Timeseries Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../Data'\n",
    "file_path = data_path + '/utilization.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format('csv').options(header=False, inferSchema=True).load(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns\n",
    "df = df.withColumnRenamed('_c0', 'event_datetime')\\\n",
    "    .withColumnRenamed('_c1', 'server_id')\\\n",
    "    .withColumnRenamed('_c2', 'cpu_utilization')\\\n",
    "    .withColumnRenamed('_c3', 'free_memory')\\\n",
    "    .withColumnRenamed('_c4', 'session_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('vw_utilization')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Timeseries Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary statistics\n",
    "\n",
    "results = spark.sql('SELECT server_id, min(cpu_utilization), max(cpu_utilization), stddev(cpu_utilization) \\\n",
    "                                FROM vw_utilization \\\n",
    "                                GROUP BY server_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+--------------------+-----------------------+\n",
      "|server_id|min(cpu_utilization)|max(cpu_utilization)|stddev(cpu_utilization)|\n",
      "+---------+--------------------+--------------------+-----------------------+\n",
      "|      148|                0.54|                0.94|    0.11451712518744131|\n",
      "|      137|                0.54|                0.94|    0.11526245077758812|\n",
      "|      133|                0.55|                0.95|    0.11534006553263144|\n",
      "|      108|                0.55|                0.95|    0.11563100171171926|\n",
      "|      101|                 0.6|                 1.0|    0.11651726263197697|\n",
      "|      115|                0.44|                0.84|    0.11569664615015006|\n",
      "|      126|                0.48|                0.88|    0.11542612970702051|\n",
      "|      103|                0.56|                0.96|    0.11617507884178278|\n",
      "|      128|                0.38|                0.78|     0.1153254132405078|\n",
      "|      122|                0.43|                0.83|    0.11563104329209034|\n",
      "+---------+--------------------+--------------------+-----------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Window function / Partition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is average server cpu utilization per server_id?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following for average cpu utilization will be calculated over server_id partition.\n",
    "basically it is another level of calculation for average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = spark.sql('SELECT event_datetime, server_id, cpu_utilization, \\\n",
    "                                avg(cpu_utilization) OVER (PARTITION BY server_id) AS avg_server_util \\\n",
    "                                FROM vw_utilization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+---------------+------------------+\n",
      "|     event_datetime|server_id|cpu_utilization|   avg_server_util|\n",
      "+-------------------+---------+---------------+------------------+\n",
      "|03/05/2019 08:07:41|      148|           0.85|0.7393840000000045|\n",
      "|03/05/2019 08:12:41|      148|           0.94|0.7393840000000045|\n",
      "|03/05/2019 08:17:41|      148|           0.89|0.7393840000000045|\n",
      "|03/05/2019 08:22:41|      148|           0.74|0.7393840000000045|\n",
      "|03/05/2019 08:27:41|      148|           0.63|0.7393840000000045|\n",
      "|03/05/2019 08:32:41|      148|           0.89|0.7393840000000045|\n",
      "|03/05/2019 08:37:41|      148|           0.77|0.7393840000000045|\n",
      "|03/05/2019 08:42:41|      148|           0.59|0.7393840000000045|\n",
      "|03/05/2019 08:47:41|      148|           0.77|0.7393840000000045|\n",
      "|03/05/2019 08:52:41|      148|           0.71|0.7393840000000045|\n",
      "|03/05/2019 08:57:41|      148|           0.85|0.7393840000000045|\n",
      "|03/05/2019 09:02:41|      148|           0.73|0.7393840000000045|\n",
      "|03/05/2019 09:07:41|      148|           0.89|0.7393840000000045|\n",
      "|03/05/2019 09:12:41|      148|           0.56|0.7393840000000045|\n",
      "|03/05/2019 09:17:41|      148|           0.59|0.7393840000000045|\n",
      "|03/05/2019 09:22:41|      148|           0.78|0.7393840000000045|\n",
      "|03/05/2019 09:27:41|      148|           0.72|0.7393840000000045|\n",
      "|03/05/2019 09:32:41|      148|            0.8|0.7393840000000045|\n",
      "|03/05/2019 09:37:41|      148|           0.55|0.7393840000000045|\n",
      "|03/05/2019 09:42:41|      148|           0.91|0.7393840000000045|\n",
      "+-------------------+---------+---------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What about the cpu utilization is over or under average server utilization?\n",
    "- difference between average utilization and every utilization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = spark.sql('SELECT event_datetime, server_id, cpu_utilization, \\\n",
    "                                avg(cpu_utilization) OVER (PARTITION BY server_id) AS avg_server_util, \\\n",
    "                                cpu_utilization - (avg(cpu_utilization) OVER (PARTITION BY server_id)) AS delta_server_util \\\n",
    "                                FROM vw_utilization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+---------------+------------------+--------------------+\n",
      "|     event_datetime|server_id|cpu_utilization|   avg_server_util|   delta_server_util|\n",
      "+-------------------+---------+---------------+------------------+--------------------+\n",
      "|03/05/2019 08:07:41|      148|           0.85|0.7393840000000045|  0.1106159999999955|\n",
      "|03/05/2019 08:12:41|      148|           0.94|0.7393840000000045| 0.20061599999999546|\n",
      "|03/05/2019 08:17:41|      148|           0.89|0.7393840000000045| 0.15061599999999553|\n",
      "|03/05/2019 08:22:41|      148|           0.74|0.7393840000000045| 6.15999999995509E-4|\n",
      "|03/05/2019 08:27:41|      148|           0.63|0.7393840000000045|-0.10938400000000448|\n",
      "|03/05/2019 08:32:41|      148|           0.89|0.7393840000000045| 0.15061599999999553|\n",
      "|03/05/2019 08:37:41|      148|           0.77|0.7393840000000045|0.030615999999995536|\n",
      "|03/05/2019 08:42:41|      148|           0.59|0.7393840000000045| -0.1493840000000045|\n",
      "|03/05/2019 08:47:41|      148|           0.77|0.7393840000000045|0.030615999999995536|\n",
      "|03/05/2019 08:52:41|      148|           0.71|0.7393840000000045|-0.02938400000000...|\n",
      "|03/05/2019 08:57:41|      148|           0.85|0.7393840000000045|  0.1106159999999955|\n",
      "|03/05/2019 09:02:41|      148|           0.73|0.7393840000000045| -0.0093840000000045|\n",
      "|03/05/2019 09:07:41|      148|           0.89|0.7393840000000045| 0.15061599999999553|\n",
      "|03/05/2019 09:12:41|      148|           0.56|0.7393840000000045|-0.17938400000000443|\n",
      "|03/05/2019 09:17:41|      148|           0.59|0.7393840000000045| -0.1493840000000045|\n",
      "|03/05/2019 09:22:41|      148|           0.78|0.7393840000000045|0.040615999999995545|\n",
      "|03/05/2019 09:27:41|      148|           0.72|0.7393840000000045|-0.01938400000000451|\n",
      "|03/05/2019 09:32:41|      148|            0.8|0.7393840000000045| 0.06061599999999556|\n",
      "|03/05/2019 09:37:41|      148|           0.55|0.7393840000000045|-0.18938400000000444|\n",
      "|03/05/2019 09:42:41|      148|           0.91|0.7393840000000045| 0.17061599999999555|\n",
      "+-------------------+---------+---------------+------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sliding Windows\n",
    "- calculating the nearest neighbors average ( this can be average of last 3 values and average of next 3 values)\n",
    "\n",
    "**In our example: we will use each 1 rows (before and after) of current row. So we need to order by datetime to make sure those events are in sequentital order**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = spark.sql('SELECT event_datetime, server_id, cpu_utilization, \\\n",
    "                                avg(cpu_utilization) OVER (PARTITION BY server_id ORDER BY event_datetime \\\n",
    "                                ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING) AS avg_server_util \\\n",
    "                                FROM vw_utilization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+---------------+------------------+\n",
      "|     event_datetime|server_id|cpu_utilization|   avg_server_util|\n",
      "+-------------------+---------+---------------+------------------+\n",
      "|03/05/2019 08:07:41|      148|           0.85|             0.895|\n",
      "|03/05/2019 08:12:41|      148|           0.94|0.8933333333333334|\n",
      "|03/05/2019 08:17:41|      148|           0.89|0.8566666666666668|\n",
      "|03/05/2019 08:22:41|      148|           0.74|0.7533333333333333|\n",
      "|03/05/2019 08:27:41|      148|           0.63|0.7533333333333334|\n",
      "|03/05/2019 08:32:41|      148|           0.89|0.7633333333333333|\n",
      "|03/05/2019 08:37:41|      148|           0.77|              0.75|\n",
      "|03/05/2019 08:42:41|      148|           0.59|              0.71|\n",
      "|03/05/2019 08:47:41|      148|           0.77|              0.69|\n",
      "|03/05/2019 08:52:41|      148|           0.71|0.7766666666666667|\n",
      "|03/05/2019 08:57:41|      148|           0.85|0.7633333333333333|\n",
      "|03/05/2019 09:02:41|      148|           0.73|0.8233333333333334|\n",
      "|03/05/2019 09:07:41|      148|           0.89|0.7266666666666667|\n",
      "|03/05/2019 09:12:41|      148|           0.56|              0.68|\n",
      "|03/05/2019 09:17:41|      148|           0.59|0.6433333333333333|\n",
      "|03/05/2019 09:22:41|      148|           0.78|0.6966666666666667|\n",
      "|03/05/2019 09:27:41|      148|           0.72|0.7666666666666666|\n",
      "|03/05/2019 09:32:41|      148|            0.8|0.6900000000000001|\n",
      "|03/05/2019 09:37:41|      148|           0.55|0.7533333333333334|\n",
      "|03/05/2019 09:42:41|      148|           0.91|0.6866666666666666|\n",
      "+-------------------+---------+---------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8933333333333334"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for the 2nd row, the sliding average will be calculated as following\n",
    "# get value from 1 row above, current row value, 1 row below | then sum it up and divide by 3\n",
    "\n",
    "(0.85 + 0.94 + 0.89) / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-datascience",
   "language": "python",
   "name": "venv-datascience"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
